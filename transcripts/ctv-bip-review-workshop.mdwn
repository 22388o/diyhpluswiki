OP\_CHECKTEMPLATEVERIFY workshop notes


# Reference materials

livestream: <https://www.pscp.tv/w/1PlJQmRZWnZJE> and tweet <https://twitter.com/JeremyRubin/status/1223664128381734912>

IRC logs: <http://gnusha.org/ctv-bip-review/>

bip119 proposal: <https://github.com/bitcoin/bips/tree/master/bip-0119.mediawiki>

proposal site: <https://utxos.org/>

branch comparison: <https://github.com/bitcoin/bitcoin/compare/master...JeremyRubin:checktemplateverify>

workshop scripts: <https://github.com/JeremyRubin/ctv-workshop>

slides: <https://docs.google.com/presentation/d/1XDiZOz52XyJc4LDSbiD9_JAaJobyF5QDGtR3O9qD7yg/edit?usp=sharing>

# Introduction

Howdy livestreamers. Can someone just tweet that the livestream has actually started? <https://twitter.com/JeremyRubin/status/1223672458516938752> Okay thanks.

Welcome to the OP\_CHECKTEMPLATEVERIFY workshop. In the email I mentioned that we do have a code of conduct. It boils down to "don't be a jerk". Knowing most of you, that should be relatively easy to do, but if not then perhaps refer to the email. If you break the code of conduct, I will evict you and tell you to leave.

I'd like to start out by making a big thank you to everyone who got us here. This has been a long project and there's a lot of support from the community that has come out from Binance, Digital Contract Design, Cypher technologies, John Pfiffer, Jim Calvin, Scaling Bitcoin, Tales from the Crypt, and DG Lab have all been big supporters of this work. I am very grateful. Also SF Bitcoin Devs has also helped with arrangements.

# Schedule

The schedule will include opening remarks, walking through the BIP, an implementation walk-through and talk about the code. We'll discuss BIP alternatives. Then we'll look at some demos and look at applications, then we'll talk about ecosystem and their support. Then we will break for lunch. Then we will discuss more ecosystem support looking at the mempool and how this new stuff will work at that layer. Then there will be an open-ended workshop session like BIP review and BIP Q&A that bleed in together. Then we will do an implementation review session to look at the code and make comments. Then I'll present my thoughts on deployment and have a discussion on what that should look like. In the evening, we have a dinner planned.

There's an IRC channel which I will try to be roughly monitoring if anybody wants to submit questions. It's also a good way for remote participants to ask questions. It's a little better than twitter for questions. For WiFi, there's slips of paper floating around. For tweeting maybe use #bip119 as the hashtag.

This is a worksho, so ask any questions that you have. It's designed to be a little more engaging and I don't want to just lecture the whole time.

# Why are we here?

The main goals today are to review bip119, we're going to learn about applications of OP\_CHECKTEMPLATEVERIFY, we're going to discuss deployment and the roadmap. We're all here because we want to improve what bitcoin is doing. That's a nice principle that we all share. I want people to leave and be excited about new projects.

I thikn OP\_CHECKTEMPLATEVERIFY is one of the most exciting things in the ecosystem right now. It's a departure from what we were able to do before, and what we're able to do after it. There's a large set of things that people haven't been able to do due to complexity, which OP\_CHECKTEMPLATEVERIFY helps solve. There's some other proposals dealing with privacy and efficiency, but you can always make bigger transactions. The transactions might be too large. In limited cases, there's no new fundamentally new capability. OP\_CHECKTEMPLATEVERIFY lets you do new things.

How are we going to prepare the ecosystem for this? What is each project going to do with OP\_CHECKTEMPLATEVERIFY?

# What kind of "better"?

What does better mean? Is it more scalable, is it more secure, are we giving more users freedoms? Are we eabling easier to design protocols? These are all ways that we can talk about better and I think everyone is going to have their own definition of better. I think we can all agree that we're generally here to make bitcoin better.

# Quick demos

I want to show a few quick demos. Let me start up some scripts and stuff. The purpose of this is not to go super indepth. You can hold your major questions. It's just to get everyone excited and thinking about what is the goal here.

I have a UX that I cobbled together. It's neat, but not yet open source. All the mechanical parts are open-source right now. There's no private codebase other than the user experience. You have these scripts in the ctv-workshop repository.

The scripts I made available-- there's one that creates a batch payment, one that generates addresses, one that generates blocks, one that sets up the chain, and one that sets up your node. You can use any of those.

This is essentially a generic transaction viewer that I built. It allows you to load in a list of transaction hex along with some metadata for coloring. It lays out all the transactions and then gives you a UX which shows you where the bits and pieces are moving.

For those of you on the livestream, I apologize that this is not publicly visible right now. Let me repoint the livestream. Now the livestreamers can join in the wonder.

These marching ants around the transaction mean it's not in a block yet. It's still pending. That's what the animation is. The thick lines show that this is part of the transactions. UTXOs are round, and transactions are square. The thin lines are ways of spending a UTXO. The thin lines are possibilities. What I'm going to do is generate a slightly simpler program to look at.

It's going to be creating a new vault. What we have is a program that allows you to say "I have a litle bit of BTC in coin storage and I want to move those coins into my hot wallet". However, I don't necessarily want to move it all at once and I don't want to have to go and access my cold wallet all the time because my cold wallet takes a week to access. So what you setup is a ratelimited control flow program that gives you a little bit of coins every once in a while.

I'm going to mine a few blocks and play a few transactions. Let me broadcast this first transaction. It takes a few seconds to get processed and picked up. So we paid into a contract, we played the first step in that contract, which created two UTXOs: one which is a withdrawal contract, and another is a continuation of the vault program. So with the withdrawal, we can send to cold storage or we can send to hot storage. Okay, let's send it to cold storage.  Once that is confirmed, the other transaction gets removed. This is an undo-send functionality. We tried to send it, we pull it back to cold storage. This is a recursive program. It can have many steps.

These contracts tend to be composable. If you have a standard CTV contract, you can take and plug it into another contract. Maybe cold storage is another CTV and you plug it into the module. These steps have to be pre-planned. We will get into the composition techniques a little bit later. I just wanted to show so that people have some context.

The next demo I want to show is a batch payment. I'm going to go into my scripts directory and now I'm going to hit "generate addresses", which outputs some test data which is a bunch of addresses which are unique and random. If you leave it running, it's like 100 of them. I'm going to paste this into the batch payment API. You can do this using the same script in the repo essentially. Then I am going to click submit. What this does is create a batch payment. I'm going to start this in the background. This is the one that I have talked about a lot.

This is in the context of congestion control. Say we have 20 addresses we're trying to pay, so if you want to broadcast a transaction that was to everyone.... if you wanted to pay everyone, you would have to do 20 work, but for each leaf node it's 3 work plus 4 work, it's like a total of 11 amount of work.

Q: What is 12 work?

A: It's the amount of chainspace required for any individual to claim their own output. The total amount of work is maybe 30% more because you have interior nodes. As an individual I want to get my own UTXO and ignore everyone else.

So say we have two transactions, one is a commitment and one is a fan-out. For any person to claim, they have to do 100 work units in terms of space to get their UTXO. This allows existing exchanges to do it.

With a radix of 1, there's a chain where one person gets paid out first. Then there's a strict ordering for who gets paid when. This simulates an annunity contract with a payout per step. You can also have an nSequence lock. This is also similar to a vault. This is a vault, and if you change what each step is a little bit, you can maybe think about having some abstract representation for some of these.

# Why isn't bitcoin as-is sufficient?

Taproot is going to make complex scripts easier, but transactions largely are not programmable. It's pretty limited in scope of what you're able to do. OP\_CHECKTEMPLATEVERIFY helps you expand that. Pre-signed transactions let you emulate this and we'll probably hear from some others here about that. But pre-signed transactions are hard to statically analyze as a third-party because you need to be a member of the n-of-n multisig to delete the key with the trusted setup.

The pre-signed transactions require interactive setups. You're going to have denial of service attacks. With OP\_CHECKTEMPLATEVERIFY, I also have to hand you a tree of transactions and the redemption conditions. You have to have extra information in your wallet about this. This is no different from secure key deletion. It's impossible to prove that a key was deleted, whereas it is possible to prove that I have given you all the OP\_CHECKTEMPLATEVERIFY information and you can check there's no alternative spending path.

It's both auditing as a party not party to the contract, and also being able to do non-interactive setup. Non-interactive setup and audit are somewhat the same thing. When you create a protocol, before you make the payment and actually broadcast it, send it to the participant and ask them to sign that you have received and then save to disk. That's an interaction at a legal liability perspective, versus interaction at a pure protocol layer. There's an important difference between the two in my opinion.

The other issue is that if you want to use CHECKSEQUENCEVERIFY locks, you can't sequence events that are serial with that. There's some complexity around that and making timings well known.

# Revisitng why we're here

I think we spent too much time to go too in depth. I want to reiterate why we're here. Maybe everyone can say why they are here and what they are excited about. If anyone wants to introduce themselves...

Secure key deletion vs opcode covenants. Prototype hardware device. Stepan Snigiriev has been teaching a class lately on this. We don't really care what the actual covenant mechanism is. The complexities around key management are substantial and independent of mechanisms.

Here's why we're not here. You can disagree about this, but here we go. We're really not here to solve all bitcoin programmability problems. I think you can make the case that it would be really cool if you can run an abstract program over your transaction state. If that's what you want, then maybe look at ethereum. That's what they want to do, and they have spent a lot of time trying to make it work. We can discuss all those issues and things we might want to do, but discussion is different from solving. We don't want to say oh there's this one use case that is kind of interesting that we could do, and we want to make a leap forward in what we're able to do with bitcoin and get that out there, and try to do some follow-up features. OP\_CHECKTEMPLATEVERIFY is composable with new things that come along, it doesn't preclude you from using new things. If you were to add OP\_CAT then you can do a lot more. If you have something that checks if another script exists as another output you can get a lot of other interesting flexibilities. OP\_CHECKTEMPLATEVERIFY can do some of this, but not everything. Really we're here to prove that OP\_CHECKTEMPLATEVERIFY is safe if it is safe, and then figure out how we're going to deployment. We're more here to see if we can move the community forward.

# BIP walkthrough

I wanted to walk through the BIP. When I say walkthrough, I'm not going to do the line-by-line.

The metaphor for bitcoin UTXOs is that UTXOs are little treasure chests that hold gold coins. Gold coins are nice because they are relatively small and uniformly shaped, such that if you have a bigger treasure chest you can fit more of these gold coins in it. What you do with a treasure chest generally is you open it up and you take the coins out, you get them all out and put some back into a new treasure chest. But if you have a covenant, essentially what you're saying is that these coins aren't just gold coins but these are actually special like a rare treasure that there's only a few of. If this is a special Jimmy Hendrix guitar, you don't want to keep it like the coins, you want to say the treasure chest should be a special treasure chest like a guitar case. This is why you want a covenant. What you're expressing is don't just treat this as random coins, have some other functionality about how to handle the coins. This is a little bit like a monad where you have some safe set of transitions and you put something inside, and then people-- you put something inside, you unwrap it like a burrito, but you want to wrap it in a safe way. If you leave it unwrapped, maybe you access it in a thread unsafe mechanism. Covenants are a good way of thinking about what are the safe way to move the assets around. The idealized covenant is a program that is attached to a transaction that observes all state in the world, everything in the world, and then says is this transaction based on looking at all the state in the world. That's really broad and arbitrary though. So we pair it down: what about covenants that just look at all the blockchain history we're able to access? So within this system, is the property still true? Well, what about only the states we define to be part of the system? Eventually you can pair down to something implementable. You can think about ethereum contracts being fundamentally like what is a turing complete covenant for moving coins around? You express all the conditions around coin movement, but they don't move directly just inside of wrappers that have programmatic constraints on what gets paid. In OP\_CHECKTEMPLATEVERIFY, we're asking what's a simple way to do this for bitcoin and says this is a guitar let's put it in a guitar case and propagate constraints like that. Saying thinsg like funds should split according to a pre-defined schedule. These can be somewhat recursive covenants too. So we're not trying to solve all the programming constraints, we don't want to build an omniscient oracle and try to get all the state in the world, but rather we're just working on something more practical and feasible. People have different expectations. You want to build something that doesn't rule out an oracle at a later step, but you want to allow the oracle to exist later. Rather than selecting from any possible transaction, maybe select from a set of 1 of 5 transactions.

With that, let's actually get into the BIP. OP\_CHECKTEMPLATEVERIFY uses OP\_NOP4 (0xb3) as a soft-fork upgrade. It's sort of like a straightforward VERIFY NOP fork. If you have a new opcode you want to add, if you change the execution semantics of the VM, like you say we're going to put this new thing on the stack and then remove it, it's fundamentally incompatible with the previous version. But if you just verify a property with an assert, then it is compatible with previous versions. So then you can make the assertion mandatory and it makes it groovy and okay. So let's make it restrictive. Every time there's nothing on the stack, it fails. If there is something on the stack, then it should be exactly 32 bytes (the size of a sha256 hash). Say there's a well-defined hash function, StandardTemplateHash, matches on the stack, and if not, it fails. We can also as a policy rule reject non-32 byte hashes.

Q: Why are you using NOP for those non-32 byte data?

A: The reason not to fail is that at some point we might want to upgrade. Right now we have no version byte, but maybe someone can add a version byte later and maybe there's a new rule for hashing the transaction. You can think about it as hash flags, like the standard template hash.

Q: Would this require a different script version?

A: Every time we want a new template, we can have a new opcode, but we have limited number of opcodes, so this lets you version the data bytes and you can use the same opcode. That's more or less like a conservative implementation detail. Segwit and taproot have done similar things where they have a default version byte, and if the version byte is not the one in the standard, then we completely define it as "nothing" for now, which gives you more flexibility down the road. Wallets that don't want to lose their coins should just conform to the standard until we define the undefined behavior.

The OP\_CHECKTEMPLATEVERIFY implementation is straightforward. It's an opcode. We check if there's enough things on the stack. We look at the size of the last element, and if it's 32 bytes, we check the standardtemplatehash using CheckStandardTemplateHash. Future upgrades can add more semantics with version bytes. But for now we exclude it from the mempool.

The specification for the template hash is a little more nuanced. There are two different use cases that we have hashes for. The first case is where all the inputs to a transaction are segwit. If they are all segwit inputs, then what you do is you don't include the script. You know that they are all zero anyway, because in segwit the scriptSig is all 0, but the witness is a separate thing that doesn't get into the hash. This saves us from efficiency later on. This makes it more clear what the expectation is. It's a reasonable optimization to make. We'll revisit the details of the hash. The key thing is how we detect if there are scriptSigs.

As a standard rule, not a consensus rule but a mempool policy rule-- if something is just a 32 byte hash with an opcode... that's standard. There's P2WSH and so on. So we add a new one, saying that a 0x20 byte hash and an opcode should be standard.

# StandardTemplateHash rationale

So we go ahead and hash the nVersion, nLockTime and maybe as I mentioned we hash the scriptSig hash. We hash the number of inputs, we hash the hash of all the sequences, we hash the number of outputs, we hash the hash of all the outputs, and we hash the input index- the index of the input that is currently executing. We could do this in any order and it would be functionally equivalent. But there's a nice optimization if we think about what is the general likelihood of what order these fields are going to be modified in. If you have a streaming hash function and you're only changing the last little bit, then you do less work. This makes it easier to write scripts and do validation. I don't know exactly how people are going to use this. I generally expect that the version is not going to change that frequently. I don't think that people are going to be using absolute locktime that much, and when they do, it will probably change infrequently. They won't be scripting absolute locktimes. Relative locktimes, on the other hand, are a different story. The number of inputs- you generally know the number of inputs, but you might want a flexible protocol where you change that, so it comes later. The sequences come next, because they might need to change based on the branch of a program being taken. The outputs hash changes a lot. The input index might change a lot, because in validation if someone expresses that it will come at a different index, we can hash everything and just change that last little bit.

Why are we doing this partial merkelization where we hash the hash of the outputs? If you want a variable length encoding of a hash, you might have ambiguities. There's two specific lengths of hashes that can be used in OP\_CHECKTEMPLATEVERIFY. It also helps with future scripting capabilities where you want to add just one more output or something, it gives you compactness property.

# Malleability

In terms of malleability, we committed to all of the data in that hash that can effect the txid, except for the outputs. It's a restricive rule. The input index cannot effect the txid. In some cases, it kind of could. Why do we want this strict rule about malleability? When you fit into a use case as a basic standard CTV template, it means you can perfectly predict what all the txids are going to be in that tree. We want to keep the txids locked down. We want no malleability. We want to know exactly what the txids are going to be. We want to just have a filter over a list of expected transactions. If you want to monitor the chain for a arbitrary covenant system, how would you know if it was something you cared about? That's kind of complex. But with a simple OP\_CHECKTEMPLATEVERIFY scheme- and you could get more complex- you can just look for txids and run some basic logic at that point, rather than looking at every transaction and carefully analyzing it. Being computationally ennumerable... in order to numerate all the conditions for a OP\_CHECKTEMPLATEVERIFY contract, it's fundamentally O(n) because there's a known number and it's countable. There's some list that someone had at some point. You can reconstruct from that list of n states and get the exact tree. With an arbitrary covenant system, that's not true, and if you had many steps then tracking that outputs, it's not clear what the txids would be on the whole path, and you would need to track recursively and regenerate all the covenant states at each depth. It's messy. It's a lot of complexity to put on wallet implementers. Instead, having a list of txids that are known ahead of time is a lot simpler. You could, if you wanted to do more complicated scanning. It's also a bloom filter issue. With a list, you can put it into a bloom filter. But with an arbitrary set of transitions, there's no bloom filter that you would be able to construct to cheaply check if a block has something you're interested in, you would have to get the full block- which isn't necessarily bad because people should have more full nodes, but it makes processing more heavy for wallets.

# scriptSigs hash

This is a weird rule. It makes an odd constraint that you fundamentally can't put a OP\_CHECKTEMPLATEVERIFY script inside of a P2SH. The reason is that.. in OP\_CHECKTEMPLATEVERIFY, you have to commit to all the scriptSigs, and the scriptSigs point to the hash of the script. It's a hash cycle. The txid of the parent transaction in the input would point to the StandardTemplateHash in the parent outut. These create some hash cycle issues. Alternatively, we can say all scriptSigs must always be zero, but that seems unfair to say that OP\_CHECKTEMPLATEVERIFY is incompatible with the whole class of outputs. So instead, it's compatible but there's more hashing if you use it in those situations. There might be some use cases where you want to use a scriptSig.

This gives us some kind of weird capabilities. If you imagine you know someone's exact scriptSig for spending an output in the transaction, then you know their signature. The signature commits to the output of that transaction. So you can kind of implement something that looks like OP\_CHECKINPUTVERIFY where you want to check that the other input is something you care about. There's some interesting things like that, but it's rather inflexible.  If people want that.  A P2SH can be committed to, like "if (spend coin with Alice's key) ELSE (spend coin with Bob's key) ENDIF CTV".

We don't have OP\_CAT right now, but it would make things more powerful. A lot of the flexibility could come with this easy script change that people are already thinking about.

# half-spend problem

OP\_CHECKTEMPLATEVERIFY is generally immune to the half-spend problem. You have to opt-in to the half-spend problem. Imagine you did not commit to the input index, and you had a OP\_CHECKTEMPLATEVERIFY script that said I am going to spend from Alice to Bob and this transaction will have 2 inputs because you want someone else to add inputs to that transaction. Maybe you wanted that. If someone else shows up with the same OP\_CHECKTEMPLATEVERIFY, then those two outputs can be combined into the same transaction because of the lack of commitment to which index the inputs appear at. This can create a problem where the OP\_CHECKTEMPLATEVERIFY is used in a way that steals coins. We should say commit to which input index these things are meant to be at.

Q: What about accounting for the obligations that each covenant requires, instead of letting one output satisfy multiple identical conditions from multiple inputs?

A: Interesting.

# Branching

You can select a branch and execute one. You have a list of StandardTemplateHashes and we have the OP\_ROLL. For non-segwit, you need OP\_FROMALTSTACK. If you're not in the segwit world, then you can get rid of the OP\_DROPs. Segwit enforces a cleanstack rule. This is not enforced in bare non-segwit script where you can leave a dirty stack. In the segwit version... the reason for the OP\_FROMALTSTACK is a little simpler than typing OP\_ROLL twice. The alternative is that you have to have an additional byte which tells you how many templates there are, and then you OP\_ROLL.  If you had more than 256 things, you would need more than one byte. Actually, more than 16. So you only have one byte literals up to 16. So it saves you a byte in a number of cases. It's conceptually simpler to explain with OP\_FROMALTSTACK. It would probably be better to imagine we had taproot, where we could have as many scripts as we want.

You can do bounded recursion too. This might be obvious. You can put one of these inside of another one. So you could have (H(pay to H(x) CTV) CTV). Each one can have timelocks; you can opt-in for singatures from different people at different times. Makes interactive protocols easier to define.

# Literal checking

A previous version of the BIP had literal checking where you would check that the element was on the stack was immediately from a push, the last push we did. This was setup in a way so that the rule could later be soft-forked out. It's kind of a little clever that we could easily remove this later if we wanted to, but enforce it at first. I got rid of this in bip119. It doesn't protect against a known issue, but it allows you to separately introduce OP\_CAT without enabling constructing things on the stack. This would make things safer. Once we get OP\_CAT, you can now construct things on the stack. Say we want OP\_CAT but not constructing templates on the hash, then maybe we want to include that literal checking rule. But a lot of developers have said it's arcane and makes writing script interpreters a pain and please don't do it. I originally included it to be more conservative. I'm generally in the camp of let's do more interesting things in bitcoin, so it's there, it's gone.

# Unspecified CTV

You can do "unspecified CTV". The scriptPubKey is CTV and the witness is H(anything). Why would you want to do this? You wouldn't want to do this in a future world where you can have a delegated witness type. Imagine a future of bitcoin where there's a script, and also another one which is delegate to another script. You could say delegate to another script, let them pick a script and execute a script. We can sign off with a key from a different template to execute. This is not "why would you do this", I'm giving you a list of things that are wacky that are things to review in the BIP not that you would necessarily want to do this. This is effectively anyonecanspend. You can already write OP\_TRUE, so this is just redundant, but it's just a weird thing I wanted to mention.

# Bare script oddity

Say you are using scriptSigs and they are committed. We can commit to inputs indirectly. It's possible to try to implement a little bit of an OP\_CHECKINPUT case, but only if the ohter inputs are non-witness or if they are witness then you can commit to the other programs but not your own input because it's already committed to itself. I would expect someone to come up with something fun to do with this. I don't think this imposes any safety issue. These are fundamentally non-recursive because you're picking an additional input you want to be with, but that input can't impose some condition on the outputs because that condition on the outputs is already enumerated inside of the template. Doesn't create crazy unbounded amounts of flexibility.

# Underfunded CTV

Why not have a list of things you are trying to audit, like Bryan said? Say you have an underfunded CTV. Maybe you have an output with 10 BTC in 10 outputs, and you only have 1 sat in the input. That transaction would not be able to broadcast. But if you have another input, you could say someone else should fund this. There could be a control path without funds, and someone else pays for it. You could imagine someone pays for lunch. I have an HTLC that lets me revoke after a week, and I put half of the BTC in one part, and someone else can showup and sign at different input indexes and combine into one clear condition. So the underfunded case is kind of interesting. You can commit to the number of people too. For accounting for each obligation in the way Bryan asked about, you would have to specify the number of obligations. You would have to introduce a lot of other functionality. The best way of doing that would in my opinion be to add some kind of explicit fee opcodes. There's other cases where you want to do a spend and "allow this much fee" or something. Unclear. This would let you write something where-- I have done SIGHASH\_SINGLE and someone else can claim the rest of this output, but not to fee. They can put it into something else, but not fee. I don't know why you would want that.

Q: How is underfunded CTV different from anyonecanpay?

A: It is anyonecanpay. I'm saying anyone can pay for half of my lunch. You both say "me" and then there's a race condition. Anyone can fulfill the other type of the contract. You can also use OP\_CHECKINPUT if you want to say this specific person. You as the second person for the underfunded case, you might want to say "me" but I don't want to express that everyone can join and pay for lunch, but I do want to say I will specifically pay for that lunch. You can have flexibility on the claim, but it is "anyone can claim" at that point.

Q: So it could be, hey, my company will you pay for my lunch, rather than will anyone pay for my lunch?

A: Yes. If you want to express that you can be spent to any output, then you should specify there should be 2 inputs can be this, and I am okay with being at input 0 or 1. If you want the unbounded case where you are spent at any index, then you need OP\_CAT to add whatever sequences are appropriate, add whatever number of inputs and outputs you want, then you just express that I want this company to have some limited set of coins. So that would be a more complex scripting primitive. While there's use cases for that, let's solve that. You can already do a lot, these are just weird edge cases you can't do as well.

Q: Is this source-specified CTV, rather than underfunded?

A: You're not  necessarily specifying the source, you could do it if you want to. You can specify the source if it weren't a taproot output. You can specify the source but not the outpoint. Specifying a specific source is a weird thing; why are you enforcing coin selection or key selection on that other wallet? It's useful for protocols, but there's no obvious use case for this. In most cases, you want anyone to add whatever output they want to the transaction in the inputs. You don't care who pays you, you just care that you got paid. It doesn't really matter what person does this.

Q: In Mike Hearn's lighthouse protocol, you could have people put money into the crowdfund and also take it back.

A: It would be complicated to do something with that many state transitions. But you could build a UTXO accumulator script where at each step adds half a bitcoin into it. If it doesn't get to the terminal state, then some other action happens, or the money is only released once you get to this big point, but then the clawback mechanism is kind of hard. There's a lot of things you can do. I think we mentioned this on bitcointalk: the wrong way to engineer OP\_CHECKTEMPLATEVERIFY is to pick a well-defined application and then figure out how to do it in OP\_CHECKTEMPLATEVERIFY. It's better to pick a use case, and find out the OP\_CHECKTEMPLATEVERIFY way of doing it, because it might look differently. You can't map on this arbitrarily signing multisig thing to this; but you could say we're using OP\_CHECKTEMPLATEVERIFY with multisig on top of this, and this is the tied-hand oracle model where oracle can only do a few things like deciding how refunds get processed but they don't get to decide who gets the money. With the chain thing, you can preclude the case where someone decides to take hteir money back. Maybe clawbacks only happen once a month or at a specific time. There's a lot of nice things you can do, but you want to figure out the UX from a OP\_CHECKTEMPLATEVERIFY native perspective.

Let's keep moving just so we can get through this session.

# Upgrading OP\_CHECKTEMPLATEVERIFY

We have left this 32-byte thing as the only defined one. If you have 33 or 34 bytes, you could define new types of OP\_CHECKTEMPLATEVERIFY as a soft-fork. The general mechanism for this is there. If you want to not commit to inputs, and propose that as a new one, that might be reasonable, but it would require a new soft-fork and there's a well-defined place for that soft-fork to go. We can add on a version byte for new semantics.

# Implementation walkthrough

I want to walk through the code. Let's do a 10,000 foot flyover. There's only a few commits.

<https://github.com/JeremyRubin/bitcoin/commits/checktemplateverify>

<https://github.com/JeremyRubin/bitcoin/pull/9/files>

You can scroll through this in the next session or on your laptop as I go.

# Add single sha256 call to CHashWriter

It normally computes the hash of the hash. One hash is enough, though.

# MOVEONLY: Move GetSequence/Output/PrevoutHash to primitives/transaction.h

This functionality belongs in the transaction primitives where this is a defined hash for the transaction. This is where we want the templatehash stuff. As a wallet implementer it makes sense to pull these things into this area.

# Refactor those utility functions

These all get the double hash. We don't need a double hash though; so let's refactor those to just define the single hash we defined earlier. Then update the pre-computed transaction data to re-hash those again. These are all just setup commits. I think taproot has something similar to this.

# Add StandardTemplateHash definition

This is an important commit. Now we're adding a StandardTemplateHash. We need a new utility function called GetScriptSigsSHA256. Then we define the way of computing StandardTemplateHash. Maybe look at the skip scriptSigs check line. The skip\_scriptSigs thing. We also want to check, are the arguments passed in the correct order? We don't have type-safe hashes for bitcoin. Maybe a type-safe hash definition where you don't put the wrong hash into the wrong place, that would be a good pull request against Bitcoin Core.

# Add SignatureChecker method for checking StandardTemplateHash

In bitcoin, we have a notion of a signature checker which handles the abstract state of the world we might need to verify when validating the transaction like what's the current chain height. We have an abstract class so that we don't need a native version of that in script. This is a natural place for this code to go. We add a method that checks this. Are things being cached? Caching can come later. Caching is not part of validation. It comes later.

# Add OP\_CHECKTEMPLATEVERIFY opcode as OP\_NOP4

We define the semantics of OP\_CHECKTEMPLATEVERIFY in this commit. The code should be the same as in the BIP. There might be differences. I use a switch instead of an if statement. It makes it easier to add upgrades in the future. Other than that, there's not a lot to check here other than the meat of the BIP.

# Precompute the StandardTemplateHash

This is like it. That's the core of the BIP. These other things are like what are we doing with the BIP and how do we make the implementation better? So now we have some hash caching. We precomputed some of these values- can we pull those in? Can we just store the hash again and not re-hash during block validation? In block validation, there's some pre-computed data structures. You pre-compute things when you receive a block you can pre-compute these batchable things of the block and run other checks. You can run through and compute these hashes. So we're adding StandardTemplateHash to this existing pre-computed issue. So it hashes all the transactions. When you're doing this, it makes sense to do this. Script validity is a different data structure. A block comes in, you look at all the transactions- and from somewhere-  you get all these precomputed things filled out in a vector, then you check validity, and then you have this other stuff. To check it, we need the hash of the transactions. All of those hashes end up making their way into here. I'm all doing is adding a new one. Let's just look at it. The Precomputed data structure is here.... you have the hashPrevouts, the hashSequence, hashOutputs, then I have the hashes that I am adding that weren't there previously. I'm just adding a few things. The only one I'm adding is the StandardTemplateHash. I'm not adding that much space compared to what else is already in there. This has the witness hash hashes. The things you need to efficiently compute the sighashes in segwit. In the case where you have standard sighash flags. If you have different sighash flags, you need something else. This is a lot of caching stuff.

# Make bare OP\_CHECKTEMPLATEVERIFY basic transactions are standard

We add a standard type for this. Adds standard type TX\_STANDARDTEMPLATE. Adding a standardness rule is there just for efficiency. As soon as you need more than one input, the overhead in the segwit world is smaller, maybe you want to add it but add it later. Are there other types- like one where we have a few different options as a standardness type? In taproot, you will already have this. You can't have vaults in barescript though if you don't add it. For vaults, just doing it in segwit anyway. This is a carveout only for bare script, in the case where you care about efficiency like a congestion control tree.

Q: can you elaborate more on why someone would want to use a bare op\_ctv basic transaction?

A: Great question. If you're doing a congestion control tree, and you're trying to pay out 1000 people and you have a tree of outputs... You're trying to decongest the network. You want the maximally efficient way for one person to claim an output. You want a minimum overhead on-chain. If you use the smallest possible script to express the OP\_CHECKTEMPLATEVERIFY constraint, then you are most efficient. Doing it inside of segwit does it with another hash and then the witness. But if it's just a single input and there's a number of outputs, and it's just this basic OP\_CHECKTEMPLATEVERIFY, then that works for this congestion control tree case with no other conditions or sign-offs you want. So I think that's important for network plumbing to have that type defined. Does that answer your question?

# OP\_CHECKTEMPLATEVERIFY deployment parameters

This is a joke for now... Can we completely define the deployment parameters and get the logic correctly? The soft-fork starts March 1st. We're not going to start March 1st. I picked this months ago. I was thinking six months ago that March 1st is reasonable but it's not going to happen. We can change those dates and then it would be defined correctly. The controversial question is do we want to use bip9 version bits? It makes sense to use a non-controversial deployment strategy.

# OP\_CHECKTEMPLATEVERIFY functional tests

This one is interesting. These are tests that make sure it's working correctly. Writing tests that don't rely on other parts of wallet functionality is some kind of painful. You're manually constructing transactions to have a bare-minimum test framework for the pull request. In a later branch, I have tests where I just call an RPC and broadcast the transactions in some order. That's much easier to deal with and look at. For the sake of testing bare functionality, here's some tests. The questions to ask are, are these tests convincing to you? Are there other edge cases that are prime to be tested, and would you like to write those tests?

# Modify script\_tests.json to enable OP\_NOP4

There's a thing saying don't use OP\_NOP4, so we modify this. This is annoying. It's pretty simple. These files are compiled into the binary to get these actually through. I don't remember what it is. Someone more familiar with that should look into that.

# Alternatives

At some point we'll have a presentation from some folks. We'll have Jacob come up and talk about his alternative. I talked a little bit about pre-signed transactions. What I want to underscore is that OP\_CHECKTEMPLATEVERIFY is not the only way to get this, but I think it's pretty good. So what are the alternatives?

## OP\_CHECKOUTPUTVERIFY

There's a far dated bitcointalk thread about covenants. I'm sure this exists in Nick Szabo's literature about smart contracts and things like that. But in terms of bitcoin, this thread was pretty much the first. There was OP\_CHECKOUTPUTVERIFY by Moser, Sirer and Eyal in 2016. It presents an extension to bitcoin scripting language that says let's add regular expressions to bitcoin script and make sure that an output matches a regular expression. It allows some self-reference, make sure I myself was included inside of myself. This is cool, and people started to think about the use cases in this limited but powerful model (MES16). They wrote code for it, but it never had traction. There's a lot of complexity in the implementation.

There's a few drawbacks with OP\_CHECKOUTPUTVERIFY. If you're trying to use this for congestion control, the script is less useful because of the size. It doesn't have a way of ensuring txid stability. You can't say, I want this exact txid because you can malleate locktimes. This was pre-segwit and pre-CHECKSEQUENCEVERIFY. To pick up MES16's OP\_CHECKOUTPUTVERIFY today, there would need to be a lot of design work. The patterns are not computationally numerable. At some point, it's O(n) because someone computed everything. For OP\_CHECKOUTPUTVERIFY, you can't do that. The wallet has to be more complex, you can't just write a bloom filter for txids.

## OP\_PUSHTXDATA

jl2012 wrote a proposal for OP\_PUSHTXDATA. This is about pulling out the relevant pieces of transactions.  There's a lot of conditions you would need to check for the obvious case of committing to everything. I think you can do anything with OP\_PUSHTXDATA maybe with one or two more fields, but it would be easy to add fields to OP\_PUSHTXDATA. If you want to make OP\_PUSHTXDATA a soft-fork and don't want to have an OP\_SUCCESSx because it's not taproot yet, there's no way to add UTXOs to the stack. So these all need verify semantics. It would be verify, and then you have another copy of the data and the script would get worse and worse.

I think this would be a reasonable way to do it, if you had a way to loop. You can kind of do looping with rolling it out in script. But it's pretty gnarly and it grows quickly. I think OP\_CHECKTEMPLATEVERIFY is superior for shipping.

# OP\_CAT + OP\_CHECKSIGFROMSTACKVERIFY

If you had both OP\_CAT + OP\_CHECKSIGFROMSTACKVERIFY you can kind of do the same things. But this is kind of complex. The scripts are really complex. I think it's unlikely that this will get deployed. I think it's disqualified because these scripts have lots of signatures and they are expensive to evaluate. You also get arbitrarily complicated scripts from this. This is probably not good for scripting. Therefore I don't think these are the best option even if strictly speaking possible.

Q: You're talking about the scaryness of the scripts that would be generated using these, but isn't there a lot of promise to having simpler script generation language that is well-verified that creates verified scripts?

A: I think that could be good. I think there's something nice about not having that being too complex. You need a formally verified script generator. The formal verification work I have seen has not been promising because you only get the properties you have proven and if you forget a property then your entire thing is completely screwed, like if you rely on OP\_CAT but you don't have the rule that checks the last input doesn't make it go above 520 bytes, and someone can pass something too large and cause it to fail... It's another thing to write covenants and pass inputs that don't trigger weird rules about how bitcoin validates state. I think simpler is better. In bitcoin the status quo is that during this audit we have everyone look over what the script is.

Q: gmaxwell said a long time ago that, with every output you're proving that you can control those funds. You're not doing computation. Ethereum went the other direction and said we should be doing on-chain computation. Instead, we should be providing proofs and not on-chain execution. I think a lot of people are allergic to doing computation on-chain.

A: I think that's a reasonable way to say it. I think OP\_CAT plus OP\_CHECKTEMPLATEVERIFY enable most of the same things that OP\_CAT + OP\_CHECKSIGFROMSTACKVERIFY would enable. OP\_CHECKTEMPLATEVERIFY enables some stuff, and CHECKSIGFROMSTACK enables a broader set of things which might be good but the chances that CHECKSIGFROMSTACKVERIFY getting through is regrettably for some people in the audience, low. But we will see, maybe in a few years we will have a nicer way. The real reason for CHECKSIGFROMSTACKVERIFY is to delegate scripts and say yes this person has signed off on this, and we can do that without adding CHECKSIGFROMSTACKVERIFY.

Q: Easier to implement OP\_PUSHTXDATA? Neither OP\_CAT or OP\_CHECKSIGFROMSTACK pushes things on the stack.

A: The argument that I am making in that point (3 on the slide), if we're comparing the scripts you're writing.... let me restate your question. You're saying CHECKSIGFROMSTACKVERIFY is still a verification OP\_NOP upgrade, and PUSHTXDATA is not soft-fork compatible in this way. CHECKSIGFROMSTACK could be added as a soft-fork but...

Q: The 3rd point seems to be saying I can do arbitrary computation on the stack with OP\_CAT and OP\_CHECKSIGFROMSTACKVERIFY ?

A: No. You have three alternatives: OP\_CHECKTEMPLATEVERIFY, OP\_PUSHTXDATA, and OP\_CHECKSIGFROMSTACKVERIFY. I've already made the argument that OP\_CHECKTEMPLATEVERIFY is better than OP\_PUSHTXDATA. The PUSHTXDATA scripts are better than the CHECKSIGFROMSTACK scripts. I'd say just using PUSHTXDATA would be better. An example of CHECKSIGFROMSTACKVERIFY would be the [Blockstream blog post](https://blockstream.com/2016/11/02/en-covenants-in-elements-alpha/). You have to serialize the transaction itself into the stack in its entirety. I don't know. You have an aneurysm when looking at it. You have to put the transaction into the stack. You serialize all of this into the script itself. The difference between CHECKSIGFROMSTACK and CHECKTEMPLATEVERIFY and TXPUSHDATA is that in CHECKSIGFROMSTACK and TXPUSHDATA you have to duplicate everything. But OP\_CHECKTEMPLATEVERIFY says don't duplicate just hash. With TXPUSHDATA, you put it on the stack not as a literal but you're saying put this on the stack from the data itself, but to enforce the covenant you have to hash it and enforce anyway. So if you're going to be adding OP\_CHECKTEMPLATEVERIFY anyway, you might as well just use OP\_CHECKTEMPLATEVERIFY. If you add TXPUSHDATA, you get the same thing in the end. The CHECKSIGFROMSTACK way of doing things is a mess by comparison. Maybe in 20 years the tooling will be better. If you look at miniscript with these other things... miniscript only works on things that are CNF like, or monotonic boolean expressions might be the exact category. As soon as you want to move transaction data into the stack and verify signatures, you have to go back and do that.

Q: Aren't you pushing a ton of the verification to the ...

A: Exactly, it's just a single hash you push on. What's nice is that those validations are cacheable outside of script validation whereas you need script validation caching for these other proposals and it's not clear these are scripts that you can cache the validity of. In validation, the scripts you write for TXPUSHDATA it's not immediately clear to me that those scripts are eligible for the script validation cache. If your transaction is not eligible for the signature validation cache, then it might be eligible for the script validation cache. PUSHTXDATA it's not clear that without a lot of work those would be eligible for either of those caches and it's not clear we would ever have a general rule to get those in there. I'm not sure what the rules are to make something eligible for those caches, but OP\_CHECKTEMPLATEVERIFY is very easily cacheable. It's an optimization in script size and also amount of work done in validation. It makes it a better and cheaper primitive to use. I could imagine you want to commit to one thing that is less than 32 byte, but that's a strawman really- when have we ever really had less than 32 bytes of conditions? What exactly are you checking- one input had a value more than something? I don't know. You're still going to have a signature in that case, anyway. Maybe a way to prove this is that any time you have a cryptographic need of authorization, you're going to need at least 32 bytes. Maybe 20 bytes for P2SH... if you wanted real efficiency.. but yeah.

# OP\_CHECKTXOUTSCRIPTHASHVERIFY

OP\_CHECKTXOUTSCRIPTHASHVERIFY is broken. It's a much weaker version of OP\_CHECKTEMPLATEVERIFY. It's a similar thing. If you like this one, you should just use OP\_CHECKTEMPLATEVERIFY.

# ANYPREVOUT (NOINPUT)

OP\_CHECKTEMPLATEVERIFY is redundant with ANYPREVOUT/ANYSCRIPT/NOINPUT. If you just had one... for all time, you could only have one or the other? Then maybe take ANYPREVOUT because you can use OP\_CHECKTEMPLATEVERIFY with it and get something similar. However, the scripts for it are worse- they are bigger and require signature validation which is going to be more expensive. They require elliptic curve operations. It's sort of similar to OP\_CHECKTEMPLATEVERIFY in other ways. There's some edge case conditions though, because you're doing a signature over something. In a segwit transaction, the first method of doing ANYPREVOUT is not eligible because you're always committing to the scriptpubkey in the signature so you need the version that is ANYPREVOUT, ANYSCRIPT, and any key, which I don't think is currently proposed. It's already not exactly ANYPREVOUT because you need an additional exemption for the signature verification algorithm, just to enable something less efficient than a purpose-built solution like OP\_CHECKTEMPLATEVERIFY.

Q: If we get any form of ANYPREVOUT or SIGHASH\_NOINPUT, you can emulate OP\_CHECKTEMPLATEVERIFY by putting a bare CHECKSIG in the redeemScript.

A: ... say there's a known public key, you have ANYPREVOUT+ANYSCRIPT, then this one should be able to validate. This one also you need the ANYPREVOUT ANYSCRIPT ANYKEY variant because otherwise you always commit to the key in Schnorr which creates a hash cycle which prevents this method from working. So you need this new NOKEY version which I don't think we want for any reason other than enabling OP\_CHECKTEMPLATEVERIFY... and just to do the same job of committing to the hash correctly... Yeah, it's off the table because of signature aggregation reasons in the Schnorr proposal...

Q: You say you need the NOPUBKEY variant. But why not just a regular CHECKSIG that doesn't use segwit? Schnorr commits to the public key in additional ways.

A: ANYPREVOUT is not proposed for non-segwit scripts. Right now it is only intended to be added to segwit. I've ignored the proposal and don't know the exact answer. Unless we add it specifically to bare script, then we wouldn't be able to use it anyway. If you did it inside of bare script, it would be incompatible with the signature aggregation use case. It might be key aggregation actually. Maybe an actual cryptographer should go look at this and come up with a proposal to do this and that's fine.. but the point is that, essentially what we're doing it is going through a lot of hoops to add a signature in our validation, but why not just add that feature anyway?

Q: I'm worried about any footguns you might try to evade with OP\_CHECKTEMPLATEVERIFY might be doable with this other covenant mechanism.

My final point about ANYPREVOUT is that at best you get something like OP\_CHECKTEMPLATEVERIFY with no ability to construct things on the stack, so no future extendibility. But you might have the half-spend issue, but maybe you commit the input indexes -that's still open for debate- but you would also not be able to upgrade without a new sighash flag and sighash type. In OP\_CHECKTEMPLATEVERIFY there's an explicit way for upgrading this stuff which is to add a new template type. There just wouldn't be an upgrade path... I'd think if you go, like, there's no such thing as the signaturehash flag... the signature hash flag maintainers you would have to convince them to add a flexible scheme for signature hashes that allows you to construct any possible amount of data to sign in the transaction, then that's going to be a really long-term project and not something that could be done in the near-term. That sort of flexibility imposes a lot of DoS considerations and then you need either special cases that those functionalities are only turned on for ANYPREVOUT ANYKEY etc... But we could just directly introduce a feature. The signature doesn't save you from any footguns because you already have the footguns already.

Q: Where does the hash cycle come from?

A: The hash cycle comes from the fact that -- for doing pubkey recovery.. Well, then you can't do this. You can use this without pubkey recovery, yes that's right because it's ANYPREVOUT ANYSCRIPT and you have a signature. In this case, if you're using a nonce point for the public key, just some well-known random number.. I don't know if you can actually use G, that might have a problem. Then you're signing with "1" as your privkey.. I don't know if that's broken? Again, that's a topic for a cryptographer.

Q: That mechanism is a covenant mechanism that achieves a lot of the same things as OP\_CHECKTEMPLATEVERIFY. If there are things prevented by OP\_CHECKTEMPLATEVERIFY, but are possible with a noncepoint public key like in ANYPREVOUT covenants, then...

A: The thing that is generally possible and is that you, you can combine ANYPREVOUT with other sighash flags which can enable you to select different parts of the transaction. I don't know if that... it's not clear to me what exactly that gives you over being able to specify one of n OP\_CHECKTEMPLATEVERIFYs. So if you're using taproot, you can specify the conditions you're interested in. One is happening at the sighash layer, and one is a literal instruction. I guess it goes back to your earlier point: with OP\_CHECKTEMPLATEVERIFY, we're saying that the transaction inherently has to satisfy this property. With adding new sighash flags, we're saying run some masking instructions on the transaction and then see if this thing fits on that pattern. Let's validate not compute, and for that philosophy then OP\_CHECKTEMPLATEVERIFY makes more sense because it's only O(log(n)) cost for that use case.

Q: OP\_CHECKTEMPLATEVERIFY is a sighash. You're taking what's normally done as a sighash and puting it over here. Why not use a sighash?

A: There's a reason for this. The way that sighash flags are defined right now, there's only 8 bytes. I think that adding more extensibility there is going to be a larger engineering project. I did consider it. It's possible to define OP\_CHECKTEMPLATEVERIFY as a new sighash flag. But then there's a question like, well you've added 128 invalid sighash flag types where we only wanted to enable this one and not these other sighash flags... you end up not being able to cache as efficiently, because you can't precompute what the OP\_CHECKTEMPLATEVERIFY hashes would be, you have to actually run the scripts in order to validate. OP\_CHECKTEMPLATEVERIFY is done in order to give you the best context-free way of evaluating whether a transaction is valid. When working with signatures, you inherently have to do something interpretative to check those signatures. With OP\_CHECKTEMPLATEVERIFY, it's just a single hash check and it can be precomputed. The strongest case to be made that ANYPREVOUT is striclty redundant, is that OP\_CHECKTEMPLATEVERIFY looks more efficient. For the use case of helping the network, caring about efficiency is a goal. We want to be able to cheaply make this. With ANYPREVOUT, unless we're using a well-defined nonce point, you don't get the compression benefits with OP\_CHECKTEMPLATEVERIFY which are known to be computed deterministically from their leaf nodes. In terms of wallets-- because the signatures...  the signatures in this case end up being, if you're using a well-known nonce point that everyone has a key for, you can recompute them. But recomputing them for validation requires more signature operations. Signing is generally slower than validating. This is just why it's a worse idea.

Q: If you have SIGHASH\_NOINPUT, you can't prevent that. So saying that OP\_CHECKTEMPLATEVERIFY is made better by this...

A: Any script that you write, it's always possible to precede it with a layer of like 10 OP\_DUPs and end it with 10 OP\_DROPs... you're free to write a less-efficient version of whatever script you want. There are many scripts that have other forms that are less efficient. If you do it the most efficient way, people will probably prefer that. Other than standardness rules, go ahead. You can write multisig as OP\_IFs, but we have CHECKMULTISIG too. This is the ability to express the same higher-order preconditions which is a mathematical object, no ordering, into two different programs. You can write two different programs in two different ways that are functionally equivalent.

Q: I am more concerned about functional inequivalence. What are the things enabled by this difference?

A: The only thing enabled as far as I understand is that you can do a covenant to any existing set of sighash flags, as long as they are compatible with NOINPUT ANYSCRIPT. Anything that is compatible with NOINPUT ANYSCRIPT, any transaction, you could then commit to. That's the difference. The problem would be, these are not well defined BIPs. I don't think there's a single definition of what are the sighash flags compatible with NOINPUT ANYSCRIPT. What are the weird edge cases you have to worry about if you're using those things? I can't directly answer that right now. You have to go read what NOINPUT ANYSCRIPT is safe with. That's one of the issues that people have with ANYPREVOUT. It's a lot of complexity and validation. People are cognizant of this; are we going to add something that is not going to preserve some key variant and now we're stuck with it because we added it at one point? I'm not going to come out and say NOINPUT ANYSCRIPT can never go in... but I would say that the set of proofs and arguments for the safety of it aren't sufficient where I have invested time enough to say this is a reasonable path... CTV could be an optimization of this.

Q: If OP\_CHECKTEMPLATEVERIFY comes without NOINPUT, then it stands on its own. But if we do NOINPUT, then we have to consider interaction with OP\_CHECKTEMPLATEVERIFY. If we get NOINPUT first, then we can do a number of things that CTV does without using CTV.

A: If you want to make the argument about getting one but not the other... The thing I would say is that if you actually want ANYPREVOUT... because ANYPREVOUT ANYSCRIPT is equivalent, and you want to decrease the amount of review burden for ANYPREVOUT ANYSCRIPT (and maybe we get just ANYPREVOUT and not ANYSCRIPT)..... the amount of difference being introduced by this new proposal is actually less, and then the security considerations are less. It makes more sense to do OP\_CHECKTEMPLATEVERIFY and then this decreases the review burden of ANYPREVOUT ANYSCRIPT and reduce the likelihood of it not getting adopted. Just pushing on one is just, it's equivalent to the sort of Schnorr/Taproot/Tapscript spiral we had where we really want Schnorr well we can also do taproot well we can also do... and as you do this, the complexity goes up. The amount of time to get Schnorr over the line is now years, whereas it could have been 2 years. If we just said Schnorr only, maybe we would have to introduce something after Schnorr, but right now it's separate enough to think about ideas where we say we're not enabling any additional features on top of ANYPREVOUT. So that's more conservative.

# Pre-signed transactions using multiparty ECDSA

I tried to get people interested in multiparty ECDSA like <https://github.com/jeremyrubin/lazuli> and it works today but there's a lot of setup assumptions. I also wasn't able to get any review for that methodology and that's where OP\_CHECKTEMPLATEVERIFY came from where I introduced something that I wanted directly.

Q: If you have Schnorr and a tree of pre-signed transactions, ... is this a tree of...?

A: The threat model is you're okay leaking your private key data, as long as you detect you leaked your private key data. In this model, you can pre-sign a bunch of transactions that you have not yet committed to. You generate everything, and then you spend to it. What's nice about this is that, not caring about whether your keys get leaked.

Q: How do you detect key leakage?

A: You detect if a signature got produced. I believe if you see a valid signature at the end state of your protocol, for my protocol, then that means your key did not get leaked. If the person worked to create the signature in this protocol, then it would not be possible to both produce a signature and a ....  You delete the key. You can safely reuse them if you have the signature. You're doing 2-p ECDSA like for lightning in theory. Bitconner has a library for that stuff.

# Pre-signed transactions with secure key deletion




Questions:

- congestion control - the tree version-- who can withdraw first? How is this specified?









